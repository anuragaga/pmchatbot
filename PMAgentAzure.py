# -*- coding: utf-8 -*-
"""Project Management Tacit Knowledge Optimized - Azure Key.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vfxs7yO4-Hi_I72-AFOPo-oPuFwkU3o7

**AZURE OPEN AI**

IN THIS NOTEBOOK, WE CREATE EMBEDDINGS ONLY FROM ANSWER COLUMN. GRAPH CREATED FROM ANSWER ONLY COLUMN IS EXPECTED TO LEARN FROM ALL THE ANSWERS AND PROVIDE RESPONSE.

Nodes and relationship and properties are selected based on relevance to project Management.
"""


from langchain_core.prompts import ChatPromptTemplate, PromptTemplate
from langchain_core.runnables import ConfigurableField
from langchain_community.vectorstores import Neo4jVector
from langchain_neo4j import GraphCypherQAChain, Neo4jGraph
from langchain.prompts import PromptTemplate
from langchain_core.prompts import ChatPromptTemplate
from langchain.schema import StrOutputParser
from langchain_openai import AzureChatOpenAI
from langchain_openai import OpenAIEmbeddings
import requests
import json
import openai
import traceback
import base64
import streamlit as st
from dotenv import load_dotenv
load_dotenv()


"""### QUERYING GRAPH DATABASE"""



client_id = st.secrets["AZURE_CLIENT_ID"]
client_secret = st.secrets["AZURE_CLIENT_SECRET"]

CISCO_OPENAI_APP_KEY = st.secrets["AZURE_CISCO_OPENAI_APP_KEY"]
# your used id
CISCO_BRAIN_USER_ID = st.secrets["CISCO_BRAIN_USER_ID"]

url = "https://id.cisco.com/oauth2/default/v1/token"

payload = "grant_type=client_credentials"
value = base64.b64encode(f'{client_id}:{client_secret}'.encode('utf-8')).decode('utf-8')
headers = {
    "Accept": "*/*",
    "Content-Type": "application/x-www-form-urlencoded",
    "Authorization": f"Basic {value}"
}

token_response = requests.request("POST", url, headers=headers, data=payload)

# print(token_response.text)

# token_response.json()["access_token"]

llm = AzureChatOpenAI(deployment_name="gpt-4o-mini",
                      azure_endpoint = 'https://chat-ai.cisco.com',
                      api_key=token_response.json()["access_token"],
                      api_version="2023-08-01-preview",
                      model_kwargs=dict(
                      user=f'{{"appkey": "{CISCO_OPENAI_APP_KEY}", "user": "{CISCO_BRAIN_USER_ID}"}}'
                    )
       )


embedding_provider = OpenAIEmbeddings(openai_api_key=st.secrets["OPENAI_API_KEY"], model = "text-embedding-ada-002")

graph = Neo4jGraph(
    url = st.secrets["NEO4J_URI"],
    username= st.secrets["NEO4J_USERNAME"],
    password= st.secrets["NEO4J_PASSWORD"]
    )

# import os
# os.environ["NEO4J_URI"] = userdata.get("NEO4J_URI")
# os.environ["NEO4J_USERNAME"] = userdata.get("NEO4J_USERNAME")
# os.environ["NEO4J_PASSWORD"] = userdata.get("NEO4J_PASSWORD")


CYPHER_GENERATION_TEMPLATE = """Task:Generate Cypher statement to query a graph database.
Instructions:
Use only the provided relationship types and properties in the schema.
Do not use any other relationship types or properties that are not provided.
Only include the generated Cypher statement in your response.

Always use case insensitive search when matching strings.

Consider plurals as single. For Example: projects is same as project and people is same as person.

Ignore names in searches and responses. Ignore addresses like Dear, Hi, Hello

Schema:
{schema}

Examples:
# Case insensitive matching for entity ids
MATCH (c:Chunk)-[:HAS_ENTITY]->(e)
WHERE e.id =~ '(?i)entityName'

# Finding documents that reference entities
MATCH (d:Document)<-[:PART_OF]-(:Chunk)-[:HAS_ENTITY]->(e)
WHERE e.id =~ '(?i)entityName'
RETURN d

The question is:
{question}"""

cypher_generation_prompt = PromptTemplate(
    template=CYPHER_GENERATION_TEMPLATE,
    input_variables=["schema", "question"],
)

cypher_chain = GraphCypherQAChain.from_llm(
    llm,
    graph=graph,
    cypher_prompt=cypher_generation_prompt,
    verbose=True,
    enhanced_schema=True,
    allow_dangerous_requests=True
)


def run_cypher(q):
  return cypher_chain.invoke(q)


from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains.retrieval import create_retrieval_chain

chunk_vector = Neo4jVector.from_existing_index(
    embedding_provider,
    graph=graph,
    index_name="chunkVector",
    embedding_node_property="textEmbedding",
    text_node_property="text",
    retrieval_query="""
// get the document
MATCH (node)-[:PART_OF]->(d:Document)
WITH node, score, d

// get the entities and relationships for the document
MATCH (node)-[:HAS_ENTITY]->(e)
MATCH p = (e)-[r]-(e2)
WHERE (node)-[:HAS_ENTITY]->(e2)

// unwind the path, create a string of the entities and relationships
UNWIND relationships(p) as rels
WITH
    node,
    score,
    d,
    collect(apoc.text.join(
        [labels(startNode(rels))[0], startNode(rels).id, type(rels), labels(endNode(rels))[0], endNode(rels).id]
        ," ")) as kg
RETURN
    node.text as text, score,
    {
        document_id: d.id,
        entities: kg
    } AS metadata
"""
)

instructions = (
    "Use only the given context to answer the question. Do not answer anything outside the context"
    "The context contains detailed information about the entities and relationships relevant to the question. "
    "Reply with an answer that includes the id of the document. Document Id are mentioned in key document_id: in the context, and other relevant information from the text."
    "Example of document Id in context is 'document_id': 'Would you allow a member of your family to be directly involved in a project you are leading?'. In this example, 'Would you allow a member of your family to be directly involved in a project you are leading?' is the document id "
    #"If you don't know the answer, say you don't know."
    "Context: {context}"
)

prompt = ChatPromptTemplate.from_messages(
    [
        ("system", instructions),
        ("human", "{input}"),
    ]
)

chunk_retriever = chunk_vector.as_retriever()
chunk_chain = create_stuff_documents_chain(llm, prompt)
chunk_retriever = create_retrieval_chain(
    chunk_retriever,
    chunk_chain
)

def find_chunk(q):
    return chunk_retriever.invoke({"input": q})


def generate_cypher_question(question):
    cypher_query = cypher_chain.generate(question)
    print("Generated Cypher Query:", cypher_query)
    return cypher_query

chat_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", "You are a project management expert providing information about project management principles, modern and new practices, experiences, tools and techniques and different project management methodologies."
        ),
        ("human", "{input}"),
    ]
)

chat = chat_prompt|llm|StrOutputParser()

from langchain.tools import Tool
tools = [
    #Tool.from_function(
    #    name = "Project Management Chat",
    #    description = "For chat related to project management priciples, practices, methodologies, tools and techniques.",
    #    func = chat.invoke
    #),

    Tool.from_function(
        name = "Search graph database",
        description = "For when you need to find information about the entities and relationship in the knowledge graph",
        func = cypher_chain.invoke
    ),

    Tool.from_function(
        name = "Search Project Management Information",
        description = "For help on project management related topic from the graph database based on information stored",
        func = find_chunk
    )
]

from langchain_neo4j import Neo4jChatMessageHistory

def get_memory(session_id):
    return Neo4jChatMessageHistory(
        session_id=session_id,
        graph=graph)

from langchain.prompts import PromptTemplate

agent_prompt = PromptTemplate.from_template("""
You are a project management expert providing guidance based on your lot of experience and learnings.
Be as helpful as possible and return as much information as possible.
Only use the information provided in the context to answer any question.
Do not answer any questions using your pre-trained knowledge.
Answer questions that only relate to project management, Scrum, Agile, Kanban and using Artificial Intelligence for project management.
Answer questions on project management principles, practices, learnings, methodologies, tools and techniques.
Add document ids in the response from which the answer has been found.
Document Id are mentioned as "document_id:" in the context.

Example of document Id in context:
'document_id': 'Would you allow a member of your family to be directly involved in a project you are leading?'
In this example, document Id is 'Would you allow a member of your family to be directly involved in a project you are leading?'



TOOLS:
------

You have access to the following tools:

{tools}

To use a tool, please use the following format:

```
Thought: Do I need to use a tool? Yes
Action: the action to take, should be one of [{tool_names}]
Action Input: the input to the action
Observation: the result of the action
```

When you have a response to say to the Human, or if you do not need to use a tool, you MUST use the format:

```
Thought: Do I need to use a tool? No
Final Answer: [your response here]
```

Begin!

Previous conversation history:
{chat_history}

New input: {input}
{agent_scratchpad}
""")



from langchain.agents import AgentExecutor, create_react_agent
from langchain_core.runnables.history import RunnableWithMessageHistory
# from langchain import hub

# agent_prompt = hub.pull("hwchase17/react-chat")
agent = create_react_agent(llm, tools, agent_prompt)
agent_executor = AgentExecutor(
    agent=agent,
    tools=tools,
    verbose=True
    )

chat_agent = RunnableWithMessageHistory(
    agent_executor,
    get_memory,
    input_messages_key="input",
    history_messages_key="chat_history",
)

### STREAMLIT INTEGRATION ####

import streamlit as st
from streamlit.runtime.scriptrunner.script_run_context import get_script_run_ctx

def get_session_id():
    return get_script_run_ctx().session_id

def generate_response(user_input):
    response = chat_agent.invoke(
        {"input": user_input},
        {"configurable": {"session_id": get_session_id()}},
        )

    return response['output']



